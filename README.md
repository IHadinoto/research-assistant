# Advanced Multi-Agent Research Assistant with RAG
## A Production-Grade LangChain Project Using Only Free Tools

**What You'll Build:** An intelligent research assistant that can analyze documents, search the web, maintain conversation history, and provide sourced answers - all running locally for $0.

**Tech Stack:**
- LangChain (orchestration framework)
- Ollama (local LLM runtime)
- FAISS (vector database)
- ChromaDB (persistent vector storage)
- LangGraph (agent orchestration)
- Python 3.10+

---

## Part 1: Environment Setup (30 minutes)

### Step 1: Install Ollama

**For macOS:**
```bash
# Download and install from ollama.ai
curl https://ollama.ai/install.sh | sh

# Verify installation
ollama --version
```

**For Linux:**
```bash
curl -fsSL https://ollama.com/install.sh | sh
```

**For Windows:**
Download the installer from ollama.ai and run it.

### Step 2: Pull Advanced Models

```bash
# Pull Llama 3.1 (8B parameters - excellent balance)
ollama pull llama3.1

# Pull a smaller embedding model for RAG
ollama pull nomic-embed-text

# Verify models are available
ollama list
```

**Why these models?**
- `llama3.1`: Fast, capable, good for reasoning
- `nomic-embed-text`: Optimized for document embeddings

### Step 3: Set Up Python Environment

```bash
# Create virtual environment
python -m venv langchain_env
source langchain_env/bin/activate  # On Windows: langchain_env\Scripts\activate

# Install core dependencies
pip install langchain langchain-community langchain-ollama
pip install chromadb faiss-cpu sentence-transformers
pip install pypdf python-dotenv langgraph
pip install beautifulsoup4 requests duckduckgo-search

# Verify installation
python -c "import langchain; print(langchain.__version__)"
```

---

## Part 2: Project Architecture

### Directory Structure
```
research-assistant/
‚îú‚îÄ‚îÄ main.py                    # Main application entry
‚îú‚îÄ‚îÄ config.py                  # Configuration settings
‚îú‚îÄ‚îÄ tools/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ web_search.py         # Web search tool
‚îÇ   ‚îú‚îÄ‚îÄ document_loader.py    # PDF/text processing
‚îÇ   ‚îî‚îÄ‚îÄ calculator.py         # Math operations
‚îú‚îÄ‚îÄ agents/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ research_agent.py     # Main research logic
‚îÇ   ‚îî‚îÄ‚îÄ memory_manager.py     # Conversation memory
‚îú‚îÄ‚îÄ embeddings/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îî‚îÄ‚îÄ vector_store.py       # Vector database manager
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ documents/            # Store your PDFs here
‚îÇ   ‚îî‚îÄ‚îÄ vectorstore/          # Persistent storage
‚îî‚îÄ‚îÄ requirements.txt
```

---

## Part 3: Building Core Components

### File 1: `config.py`
```python
"""
Configuration module for the research assistant.
Uses environment variables for flexibility.
"""
import os
from pathlib import Path

# Base directories
BASE_DIR = Path(__file__).parent
DATA_DIR = BASE_DIR / "data"
DOCS_DIR = DATA_DIR / "documents"
VECTOR_DIR = DATA_DIR / "vectorstore"

# Create directories if they don't exist
DOCS_DIR.mkdir(parents=True, exist_ok=True)
VECTOR_DIR.mkdir(parents=True, exist_ok=True)

# Model configurations
OLLAMA_BASE_URL = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
LLM_MODEL = os.getenv("LLM_MODEL", "llama3.1")
EMBEDDING_MODEL = os.getenv("EMBEDDING_MODEL", "nomic-embed-text")

# RAG settings
CHUNK_SIZE = 1000
CHUNK_OVERLAP = 200
TOP_K_RESULTS = 4

# Agent settings
MAX_ITERATIONS = 10
TEMPERATURE = 0.7
MAX_TOKENS = 2048

# Memory settings
CONVERSATION_MEMORY_KEY = "chat_history"
MAX_MEMORY_MESSAGES = 10
```

### File 2: `embeddings/vector_store.py`
```python
"""
Advanced vector store manager with persistent storage.
Handles document embeddings and similarity search.
"""
from langchain_community.vectorstores import Chroma
from langchain_ollama import OllamaEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyPDFLoader, TextLoader, DirectoryLoader
from typing import List
import config

class VectorStoreManager:
    """
    Manages document embeddings and vector storage with persistence.
    """
    
    def __init__(self):
        """Initialize embeddings and vector store."""
        self.embeddings = OllamaEmbeddings(
            model=config.EMBEDDING_MODEL,
            base_url=config.OLLAMA_BASE_URL
        )
        
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=config.CHUNK_SIZE,
            chunk_overlap=config.CHUNK_OVERLAP,
            length_function=len,
            separators=["\n\n", "\n", ".", "!", "?", ",", " ", ""]
        )
        
        self.vectorstore = None
        self._initialize_vectorstore()
    
    def _initialize_vectorstore(self):
        """Initialize or load existing vector store."""
        try:
            # Try to load existing vectorstore
            self.vectorstore = Chroma(
                persist_directory=str(config.VECTOR_DIR),
                embedding_function=self.embeddings,
                collection_name="research_docs"
            )
            print(f"‚úì Loaded existing vector store with {self.vectorstore._collection.count()} documents")
        except:
            # Create new vectorstore
            self.vectorstore = Chroma(
                persist_directory=str(config.VECTOR_DIR),
                embedding_function=self.embeddings,
                collection_name="research_docs"
            )
            print("‚úì Created new vector store")
    
    def load_documents(self, file_path: str = None) -> int:
        """
        Load documents from file or directory and add to vector store.
        
        Args:
            file_path: Path to file or directory. If None, loads from config.DOCS_DIR
            
        Returns:
            Number of document chunks added
        """
        if file_path is None:
            file_path = str(config.DOCS_DIR)
        
        # Determine loader based on file type
        if file_path.endswith('.pdf'):
            loader = PyPDFLoader(file_path)
        elif file_path.endswith('.txt'):
            loader = TextLoader(file_path)
        else:
            # Load all files from directory
            loader = DirectoryLoader(
                file_path,
                glob="**/*.pdf",
                loader_cls=PyPDFLoader,
                show_progress=True
            )
        
        # Load and split documents
        documents = loader.load()
        chunks = self.text_splitter.split_documents(documents)
        
        # Add to vector store
        if chunks:
            self.vectorstore.add_documents(chunks)
            print(f"‚úì Added {len(chunks)} document chunks to vector store")
        
        return len(chunks)
    
    def similarity_search(self, query: str, k: int = config.TOP_K_RESULTS) -> List:
        """
        Perform similarity search on vector store.
        
        Args:
            query: Search query
            k: Number of results to return
            
        Returns:
            List of relevant documents
        """
        if self.vectorstore._collection.count() == 0:
            return []
        
        results = self.vectorstore.similarity_search_with_score(query, k=k)
        return results
    
    def as_retriever(self):
        """Return vectorstore as a retriever for chain integration."""
        return self.vectorstore.as_retriever(
            search_kwargs={"k": config.TOP_K_RESULTS}
        )
    
    def clear_store(self):
        """Clear all documents from vector store."""
        self.vectorstore.delete_collection()
        self._initialize_vectorstore()
        print("‚úì Vector store cleared")
```

### File 3: `tools/web_search.py`
```python
"""
Web search tool using DuckDuckGo for privacy-focused searches.
"""
from langchain.tools import Tool
from langchain_community.utilities import DuckDuckGoSearchAPIWrapper
from typing import Optional

class WebSearchTool:
    """
    Advanced web search tool with result filtering and ranking.
    """
    
    def __init__(self, max_results: int = 5):
        """
        Initialize web search tool.
        
        Args:
            max_results: Maximum number of search results to return
        """
        self.search = DuckDuckGoSearchAPIWrapper()
        self.max_results = max_results
    
    def search_web(self, query: str) -> str:
        """
        Perform web search and return formatted results.
        
        Args:
            query: Search query
            
        Returns:
            Formatted search results
        """
        try:
            results = self.search.results(query, max_results=self.max_results)
            
            if not results:
                return "No results found for this query."
            
            formatted_results = []
            for idx, result in enumerate(results, 1):
                formatted_results.append(
                    f"{idx}. {result.get('title', 'N/A')}\n"
                    f"   URL: {result.get('link', 'N/A')}\n"
                    f"   Snippet: {result.get('snippet', 'N/A')}\n"
                )
            
            return "\n".join(formatted_results)
        
        except Exception as e:
            return f"Search error: {str(e)}"
    
    def as_tool(self) -> Tool:
        """Return as LangChain Tool for agent integration."""
        return Tool(
            name="web_search",
            description=(
                "Useful for searching the web for current information, news, "
                "facts, or any topic not in your knowledge base. "
                "Input should be a search query string."
            ),
            func=self.search_web
        )
```

### File 4: `tools/calculator.py`
```python
"""
Calculator tool for mathematical operations.
"""
from langchain.tools import Tool
import re

class CalculatorTool:
    """
    Safe mathematical calculator for agent use.
    """
    
    def calculate(self, expression: str) -> str:
        """
        Safely evaluate mathematical expressions.
        
        Args:
            expression: Mathematical expression to evaluate
            
        Returns:
            Result of calculation or error message
        """
        try:
            # Remove any non-mathematical characters for safety
            safe_expr = re.sub(r'[^0-9+\-*/().\s]', '', expression)
            
            # Evaluate expression
            result = eval(safe_expr, {"__builtins__": {}}, {})
            return f"Result: {result}"
        
        except Exception as e:
            return f"Calculation error: {str(e)}"
    
    def as_tool(self) -> Tool:
        """Return as LangChain Tool for agent integration."""
        return Tool(
            name="calculator",
            description=(
                "Useful for performing mathematical calculations. "
                "Input should be a mathematical expression like '25 * 4' or '(100 + 50) / 2'."
            ),
            func=self.calculate
        )
```

### File 5: `agents/memory_manager.py`
```python
"""
Conversation memory management with context window optimization.
"""
from langchain.memory import ConversationBufferWindowMemory
from langchain_community.chat_message_histories import ChatMessageHistory
import config

class MemoryManager:
    """
    Manages conversation history with intelligent context pruning.
    """
    
    def __init__(self):
        """Initialize memory with window-based retention."""
        self.memory = ConversationBufferWindowMemory(
            k=config.MAX_MEMORY_MESSAGES,
            memory_key=config.CONVERSATION_MEMORY_KEY,
            return_messages=True,
            output_key="output"
        )
    
    def add_interaction(self, user_input: str, assistant_output: str):
        """
        Add a user-assistant interaction to memory.
        
        Args:
            user_input: User's message
            assistant_output: Assistant's response
        """
        self.memory.save_context(
            {"input": user_input},
            {"output": assistant_output}
        )
    
    def get_history(self) -> str:
        """
        Get formatted conversation history.
        
        Returns:
            Formatted string of conversation history
        """
        return self.memory.load_memory_variables({}).get(
            config.CONVERSATION_MEMORY_KEY, []
        )
    
    def clear(self):
        """Clear all conversation history."""
        self.memory.clear()
```

### File 6: `agents/research_agent.py`
```python
"""
Advanced research agent with multi-tool capabilities.
"""
from langchain_ollama import ChatOllama
from langchain.agents import AgentExecutor, create_react_agent
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA
from typing import List, Dict
import config

class ResearchAgent:
    """
    Multi-tool research agent with RAG capabilities.
    """
    
    def __init__(self, tools: List, vector_store_manager, memory_manager):
        """
        Initialize research agent.
        
        Args:
            tools: List of LangChain tools
            vector_store_manager: Vector store manager instance
            memory_manager: Memory manager instance
        """
        self.llm = ChatOllama(
            model=config.LLM_MODEL,
            base_url=config.OLLAMA_BASE_URL,
            temperature=config.TEMPERATURE,
            num_predict=config.MAX_TOKENS
        )
        
        self.tools = tools
        self.vector_store = vector_store_manager
        self.memory = memory_manager
        
        # Create RAG chain for document queries
        self.rag_chain = RetrievalQA.from_chain_type(
            llm=self.llm,
            chain_type="stuff",
            retriever=self.vector_store.as_retriever(),
            return_source_documents=True
        )
        
        # Create agent with tools
        self.agent = self._create_agent()
    
    def _create_agent(self):
        """Create ReAct agent with tools."""
        
        # Advanced ReAct prompt template
        prompt = PromptTemplate.from_template("""
You are an advanced research assistant with access to multiple tools and a knowledge base.

You have access to the following tools:
{tools}

Tool Names: {tool_names}

Use this format:

Question: the input question you must answer
Thought: you should always think about what to do
Action: the action to take, should be one of [{tool_names}]
Action Input: the input to the action
Observation: the result of the action
... (this Thought/Action/Action Input/Observation can repeat N times)
Thought: I now know the final answer
Final Answer: the final answer to the original input question

Previous conversation:
{chat_history}

Question: {input}
{agent_scratchpad}
""")
        
        # Create agent
        agent = create_react_agent(
            llm=self.llm,
            tools=self.tools,
            prompt=prompt
        )
        
        # Create executor
        agent_executor = AgentExecutor(
            agent=agent,
            tools=self.tools,
            verbose=True,
            max_iterations=config.MAX_ITERATIONS,
            handle_parsing_errors=True,
            memory=self.memory.memory
        )
        
        return agent_executor
    
    def query_documents(self, question: str) -> Dict:
        """
        Query the document knowledge base using RAG.
        
        Args:
            question: Question to ask about documents
            
        Returns:
            Dictionary with answer and sources
        """
        if self.vector_store.vectorstore._collection.count() == 0:
            return {
                "answer": "No documents loaded in knowledge base.",
                "sources": []
            }
        
        result = self.rag_chain.invoke({"query": question})
        
        return {
            "answer": result["result"],
            "sources": [doc.metadata for doc in result.get("source_documents", [])]
        }
    
    def research(self, query: str) -> str:
        """
        Perform comprehensive research using all available tools.
        
        Args:
            query: Research query
            
        Returns:
            Research results
        """
        try:
            response = self.agent.invoke({"input": query})
            return response["output"]
        except Exception as e:
            return f"Research error: {str(e)}"
```

### File 7: `main.py`
```python
"""
Main application entry point for the Advanced Research Assistant.
"""
from embeddings.vector_store import VectorStoreManager
from agents.memory_manager import MemoryManager
from agents.research_agent import ResearchAgent
from tools.web_search import WebSearchTool
from tools.calculator import CalculatorTool
import config
from pathlib import Path

class ResearchAssistant:
    """
    Main application class orchestrating all components.
    """
    
    def __init__(self):
        """Initialize all components."""
        print("üöÄ Initializing Advanced Research Assistant...")
        
        # Initialize vector store
        print("üìö Loading vector store...")
        self.vector_store = VectorStoreManager()
        
        # Initialize memory
        print("üß† Setting up conversation memory...")
        self.memory = MemoryManager()
        
        # Initialize tools
        print("üîß Configuring tools...")
        web_search = WebSearchTool(max_results=5)
        calculator = CalculatorTool()
        
        self.tools = [
            web_search.as_tool(),
            calculator.as_tool()
        ]
        
        # Initialize agent
        print("ü§ñ Creating research agent...")
        self.agent = ResearchAgent(
            tools=self.tools,
            vector_store_manager=self.vector_store,
            memory_manager=self.memory
        )
        
        print("‚úÖ Research Assistant ready!\n")
    
    def load_documents(self):
        """Load documents from the data directory."""
        docs_path = config.DOCS_DIR
        
        if not any(docs_path.iterdir()):
            print(f"‚ö†Ô∏è  No documents found in {docs_path}")
            print(f"   Place your PDF or TXT files in this directory to load them.")
            return
        
        print(f"üìñ Loading documents from {docs_path}...")
        count = self.vector_store.load_documents()
        
        if count > 0:
            print(f"‚úì Successfully loaded {count} document chunks")
        else:
            print("‚ö†Ô∏è  No documents were loaded")
    
    def query_documents(self, question: str):
        """Query the document knowledge base."""
        print(f"\nüîç Searching documents for: '{question}'")
        result = self.agent.query_documents(question)
        
        print(f"\nüìÑ Answer: {result['answer']}")
        
        if result['sources']:
            print("\nüìö Sources:")
            for idx, source in enumerate(result['sources'], 1):
                print(f"   {idx}. {source.get('source', 'Unknown')}")
    
    def research(self, query: str):
        """Perform comprehensive research."""
        print(f"\nüî¨ Researching: '{query}'")
        print("=" * 60)
        
        response = self.agent.research(query)
        
        print("=" * 60)
        print(f"\nüí° Final Answer:\n{response}\n")
    
    def interactive_mode(self):
        """Run in interactive chat mode."""
        print("\n" + "=" * 60)
        print("INTERACTIVE MODE - Type 'quit' to exit")
        print("Commands:")
        print("  /docs [query]  - Search your document knowledge base")
        print("  /load          - Reload documents from data directory")
        print("  /clear         - Clear conversation history")
        print("  /help          - Show this help message")
        print("=" * 60 + "\n")
        
        while True:
            try:
                user_input = input("You: ").strip()
                
                if not user_input:
                    continue
                
                if user_input.lower() == 'quit':
                    print("üëã Goodbye!")
                    break
                
                if user_input.startswith('/docs'):
                    query = user_input[6:].strip()
                    if query:
                        self.query_documents(query)
                    else:
                        print("‚ö†Ô∏è  Please provide a query after /docs")
                    continue
                
                if user_input == '/load':
                    self.load_documents()
                    continue
                
                if user_input == '/clear':
                    self.memory.clear()
                    print("‚úì Conversation history cleared")
                    continue
                
                if user_input == '/help':
                    print("\nCommands:")
                    print("  /docs [query]  - Search your document knowledge base")
                    print("  /load          - Reload documents from data directory")
                    print("  /clear         - Clear conversation history")
                    print("  /help          - Show this help message\n")
                    continue
                
                # Regular research query
                self.research(user_input)
                
            except KeyboardInterrupt:
                print("\nüëã Goodbye!")
                break
            except Exception as e:
                print(f"‚ùå Error: {str(e)}")

def main():
    """Main entry point."""
    assistant = ResearchAssistant()
    
    # Load any documents in the data directory
    assistant.load_documents()
    
    # Enter interactive mode
    assistant.interactive_mode()

if __name__ == "__main__":
    main()
```

---

## Part 4: Running Your Advanced Research Assistant

### Quick Start

1. **Ensure Ollama is running:**
```bash
ollama serve
```

2. **Run the assistant:**
```bash
python main.py
```

### Example Interactions

**Example 1: Web Search + Calculation**
```
You: What is the current population of Japan and what would that be divided by 47 prefectures?

[Agent will use web_search to find Japan's population, then calculator to divide]
```

**Example 2: Document Query**
```
You: /docs What are the key findings about climate change in the reports?

[Agent searches your loaded PDFs using RAG]
```

**Example 3: Multi-Step Research**
```
You: Compare the GDP of the top 3 Asian economies and calculate the percentage difference

[Agent will:
1. Search for current GDP data
2. Identify top 3 economies
3. Use calculator for percentage calculations
4. Synthesize comprehensive answer]
```

---

## Part 5: Advanced Features to Add

### Feature 1: Conversation Export
```python
def export_conversation(self, filename: str):
    """Export conversation history to file."""
    history = self.memory.get_history()
    with open(filename, 'w') as f:
        for msg in history:
            f.write(f"{msg.type}: {msg.content}\n\n")
```

### Feature 2: Custom Tool Creation
```python
# Add to tools/ directory
class CustomAPITool:
    """Template for creating custom API integration tools."""
    
    def call_api(self, params: str) -> str:
        # Your API logic here
        pass
    
    def as_tool(self):
        return Tool(
            name="custom_api",
            description="Description of what your API does",
            func=self.call_api
        )
```

### Feature 3: Document Summarization
```python
def summarize_document(self, doc_path: str) -> str:
    """Generate comprehensive document summary."""
    loader = PyPDFLoader(doc_path)
    docs = loader.load()
    
    prompt = """Provide a comprehensive summary of this document:
    
    {text}
    
    Summary:"""
    
    chain = LLMChain(llm=self.agent.llm, prompt=PromptTemplate.from_template(prompt))
    return chain.run(text=docs[0].page_content[:4000])
```

---

## Part 6: Making It Sound More Advanced

### Professional Enhancements

1. **Add Logging:**
```python
import logging

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('research_assistant.log'),
        logging.StreamHandler()
    ]
)
```

2. **Add Performance Metrics:**
```python
import time

def timed_query(self, query: str):
    """Execute query with performance tracking."""
    start = time.time()
    result = self.agent.research(query)
    elapsed = time.time() - start
    
    print(f"\n‚è±Ô∏è  Query completed in {elapsed:.2f} seconds")
    return result
```

3. **Add Async Processing:**
```python
import asyncio

async def async_research(self, queries: List[str]):
    """Process multiple queries concurrently."""
    tasks = [asyncio.to_thread(self.research, q) for q in queries]
    return await asyncio.gather(*tasks)
```

---

## Part 7: Testing Your System

### Test Script
```python
def run_tests():
    """Comprehensive system tests."""
    assistant = ResearchAssistant()
    
    print("üß™ Running system tests...\n")
    
    # Test 1: Calculator
    print("Test 1: Calculator Tool")
    assistant.research("What is 1234 * 5678?")
    
    # Test 2: Web Search
    print("\nTest 2: Web Search Tool")
    assistant.research("What are the latest developments in AI?")
    
    # Test 3: Memory
    print("\nTest 3: Conversation Memory")
    assistant.research("My favorite color is blue")
    assistant.research("What is my favorite color?")
    
    print("\n‚úÖ All tests completed!")

if __name__ == "__main__":
    run_tests()
```

---

## Part 8: Next-Level Additions

### 1. **Multi-Agent Collaboration**
Create specialized agents (researcher, writer, critic) that work together

### 2. **Streaming Responses**
Implement token-by-token streaming for real-time feedback

### 3. **Web Interface**
Build a Flask/FastAPI frontend with a chat UI

### 4. **Fine-tuning Integration**
Fine-tune Ollama models on domain-specific data

### 5. **Advanced RAG Techniques**
- Hypothetical Document Embeddings (HyDE)
- Parent Document Retrieval
- Multi-query retrieval
- Contextual compression

---

## Troubleshooting

**Problem: Ollama not responding**
```bash
# Check if Ollama is running
curl http://localhost:11434/api/tags

# Restart Ollama
pkill ollama && ollama serve
```

**Problem: Out of memory**
```bash
# Use smaller models
ollama pull llama3.1:8b  # Instead of larger variants
```

**Problem: Slow responses**
```python
# Reduce context window
config.MAX_TOKENS = 1024
config.CHUNK_SIZE = 500
```

---

## Summary

You've built a production-grade research assistant with:
- ‚úÖ Local LLM inference (completely free)
- ‚úÖ RAG for document analysis
- ‚úÖ Multi-tool agent system
- ‚úÖ Conversation memory
- ‚úÖ Web search capabilities
- ‚úÖ Extensible architecture

**Total cost: $0.00** üéâ

This system rivals commercial solutions and demonstrates enterprise-level LangChain proficiency!
